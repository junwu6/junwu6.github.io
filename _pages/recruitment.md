---
layout: archive
title: # ""
permalink: /recruitment/
author_profile: true
---

Thanks for your interest. If you are interested in joining my team, please apply to the MSU CSE [graduate program](https://at3.cse.msu.edu/Students/Future_Grad/HowToApply.php) and mention my name in your application statement. Students with CS, math, and EE backgrounds are particularly encouraged to apply!

Here are several optional research projects in my team.

* **Graph Limit theory**. It models graph data in the limit as the number of nodes approaches infinity, and can also serve as a generative model for graph sequences. Our research aims to understand the convergence, stability, and generalization performance of modern graph learning methods.

* **Flow Matching**. Flow matching is a flexible and scalable framework for learning probabilistic maps between distributions. We investigate its theoretical foundations for handling distribution shifts between training and testing environments.

* **AI for Math**. This focuses on using machine learning to assist in mathematical reasoning, theorem proving, and discovery. We are interested in both building tools that support human mathematicians and studying how AI systems can develop mathematical understanding.



<!--
* **Hypothesis Transfer Learning**. This framework focuses on optimizing the learning function on the target task using source hypotheses. Our research aims to bridge the gap between theoretical understanding and practical algorithms in understanding the transferability of pre-trained neural networks, such as large language models and vision-language models.

* **Graphon theory**. It models graph data in the limit as the number of nodes approaches infinity, and can also serve as a generative model for graph sequences. Our research aims to understand the convergence, stability, and generalization performance of modern graph learning methods, such as graph neural networks and graph transformers.
  
* **Uncertainty Quantification under Distribution Shifts**. Our research aims to develop conformal prediction or Bayesian learning frameworks to rigorously quantify the prediction uncertainty of domain adaptation techniques under distribution shifts.

* Out-of-distribution generalization. The goal is to learn a domain-agnostic prediction function from training domains such that the learned function performs well on new unseen domains. There are two key research challenges: learning a good foundation model from training domains and identifying the generalization bounds.

* Fairness and Robustness of LLMs. It seeks to determine whether black-box large language models (LLMs) consistently deliver fair and robust results to a diverse range of users and customers. 

* Transparency of transfer learning. It explains what knowledge is being transferred in the transfer learning process, e.g., what essential knowledge of pre-trained LLMs can be leveraged for user-specific downstream tasks, and how to efficiently find a pre-trained LLM from thousands of candidates in HuggingFace. Another relevant problem is the uncertainty quantification of the transfer learning models, which determines how confident the models are in their predictions.

* Fundamental trade-off between prediction accuracy and trustworthy properties under distribution shifts. This aims to theoretically understand how trustworthy properties (e.g., privacy, fairness, etc.) affect the transfer learning performance.
-->
